{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](../graphics/solutions-microsoft-logo-small.png)\n",
    "\n",
    "# Python for Data Professionals\n",
    "\n",
    "## 03 Working with Data\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n",
    "\n",
    "<dl>\n",
    "  <dt>Course Outline</dt>\n",
    "  <dt>1 - Overview and Course Setup</dt>\n",
    "  <dt>2 - Programming Basics</dt>\n",
    "  <dt>3 Working with Data <i>(This section)</i></dt>\n",
    "    <dd>3.1 Data Types</dd>\n",
    "    <dd>3.2 Data Ingestion</dd>\n",
    "    <dd>3.3 Data Inspection</dd>\n",
    "    <dd>3.4 Graphing</dd>\n",
    "    <dd>3.5 Machine Learning and AI</dd>\n",
    "  <dt>4 Deployment and Environments</dt>\n",
    "<dl>\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with data is the main part of this course. This section will be quite a bit longer than what you have done so far, and the Activities will be harder. Remember to use the cheat-sheets and other references in the `./assets` course directory, because not everything you need to know will be in the course explanation. You'll need to dig a bit more and experiment, use the `help()` function, and do a bit of researching to figure out how to complete the Activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.1 Data Types</b></p>\n",
    "\n",
    "In most any language, after the Data Professional learns how to use help, they want to find out what data types the language supports, and how the language works with them. You covered the way Python works with data in the last module (under the topic *Operators*), so now you need to figure out the types of data Python can work with.\n",
    "\n",
    "Note that the data types you'll see next are the ones built-in to the Python language. Just like a Data Platform will often take a \"primitive\" data type and build on that with libraries, Python will do the same thing. You'll cover that in more depth in a moment.\n",
    "\n",
    "Python has 5 standard data type \"families\":\n",
    "\n",
    "- Numbers\n",
    "- Strings\n",
    "- Lists\n",
    "- Tuples\n",
    "- Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Numbers</b></p>\n",
    "\n",
    "Numbers contain the following types:\n",
    "\n",
    "- int (signed integer)\n",
    "- long (long integers in either decimal, octal or hex)\n",
    "- float (real floating point numbers)\n",
    "- complex (integers in the range of 0-255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Strings</b></p>\n",
    "\n",
    "Strings are ASCII characters within a single quote, double quote, or if you want to span a line, triple quotes. They are treated as an array of sorts, so if you do this:\n",
    "\n",
    "`myName = \"Buck\"`\n",
    "\n",
    "Then you can do this:\n",
    "\n",
    "`print(myName[0])`\n",
    "\n",
    "And you get back this:\n",
    "\n",
    "<pre>B</pre>\n",
    "\n",
    "Oh, and there are all kinds of formatting options you have with strings. [Check those out here](https://pyformat.info/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buck\n"
     ]
    }
   ],
   "source": [
    "# Try it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Lists</b></p>\n",
    "\n",
    "Lists are arrays - and you're not limited to a single dimension. You define them by enclosing the values in square brackets.\n",
    "\n",
    "Here's a list:\n",
    "\n",
    "`myList = [0, 1, 2]`\n",
    "\n",
    "And now you can decorate that with data:\n",
    "\n",
    "`myList[0] = \"One\"`\n",
    "\n",
    "`myList[1] = \"Two\"`\n",
    "\n",
    "`myList[2] = \"Three\"`\n",
    "\n",
    "`print(myList)`\n",
    "\n",
    "`print(myList[2])`\n",
    "\n",
    "And so on. There are also ranges, loops, and methods you can use on lists - [more on that here](https://www.tutorialspoint.com/python/python_lists.htm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Tuples</b></p>\n",
    "\n",
    "Tuples are similar to Lists, but are immutable - you can't extend or shrink them dynamically. Think of them as a readable SQL Table. You define a tuple with parenthesis rather than square brackets.\n",
    "\n",
    "Use a Tuple when you want to \"protect\" the data structure so that no one changes the structure after you define it. You'll see some real-world examples throughout this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Dictionaries</b></p>\n",
    "\n",
    "Dictionaries are Key-Value Pair (KVP) data. You set these up with a curly-brace, the key, a colon, and then the value, like this:\n",
    "\n",
    "`myDict = {1: \"Buck\", 2: \"Jane\", 3: \"Jim\"}`\n",
    "\n",
    "Now you can work with them by the key or the value. For instance, to show the value for key 1, it's simply: \n",
    "\n",
    "`myDict[1]`\n",
    "\n",
    "Or to find the key for Buck, you simply type this:\n",
    "\n",
    "`myDict[\"Buck\"]`\n",
    "\n",
    "Dictionaries are used quite frequently in Python, so you should take some time to [read up on them here](https://docs.python.org/2/tutorial/datastructures.html#dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/aml-logo.png\"><b>Activity - Programming basics</b></p>\n",
    "\n",
    "Open the **03_WorkingWithData.py** file and enter the code you find for section 3.1. The exercises will be marked out using comments:\n",
    "\n",
    "`# <TODO> - 3.1 `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_WorkingWithData.py\n",
    "# Purpose: Exercise files for Python for Data Professionals course, section 3\n",
    "# Author: Buck Woody\n",
    "# Credits and Sources: Inline\n",
    "# Last Updated: 02 July 2018\n",
    "\n",
    "# <TODO> - 3.1 Data Types\n",
    "\n",
    "# Create a variable called MyName and set it to your name. \n",
    "# Print out the middle two letters of the variable:\n",
    "\n",
    "# Create a new variable of a 3-digit number. Print out the data type for the variable:\n",
    "\n",
    "# Change the previous variable to text. Print the data type for the variable:\n",
    "\n",
    "# Create a list structure with three numbers in it, add two of the numbers, print the result:\n",
    "\n",
    "# Create a Dictionary structure with three values using keys of 1, 2 and 3.\n",
    "# Query for the value of key 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.2 Side-track: Working with Libraries for Data</b></p>\n",
    "\n",
    "Python includes most of the functions you need to read data from files, work with them in memory and so on in the base installation. However, There is a way to add in to the functions you have for your code, using *Libraries*. Libraries are code someone else has written that you add in to your program from the start, using an `import` statement. You'll cover more information on working with Libraries (sometimes referred to as Modules or Packages, but more correctly Libraries) in a future lesson, but data \"wrangling\" (importing, manipulating and exporting) usually involves adding in at least one or two Libraries, so you'll cover that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>NumPy</b></p>\n",
    "\n",
    "*NOTE: You'll need to install both NumPy and Pandas before you can use them. You will cover that in a later lesson - your pre-requisites included this installation for now.*\n",
    "\n",
    "To work with numeric data, the first library you should become familiar with is *NumPy* (Numerical Python). The primary structure in NumPy is the *array*.\n",
    "\n",
    "To load the library, use the `import` statement with an optional \"alias\" of np:\n",
    "\n",
    "`import numpy as np`\n",
    "\n",
    "Now when you reference NumPy's methods and properties, you can use the shorter `np` label.\n",
    "\n",
    "It's simple enough to create and work with an array, now that you have the library loaded. This code creates a 2-dimensional NumPy array, and sets the values to integer:\n",
    "\n",
    "`x = np.array([(1,2,3), (4,5,6)], dtype = int)`\n",
    "\n",
    "The next important concept in NumPy is that the array is actually a set of pointers, involving four main components:\n",
    "\n",
    "- *data* : The memory address of the first byte in the array\n",
    "\n",
    "- *dtype* : The type of the elements in the array\n",
    "\n",
    "- *shape* : The layout of the array\n",
    "\n",
    "- *strides* : The number of bytes skipped in memory to go to the next element of the array\n",
    "\n",
    "Here are those properties in action:\n",
    "\n",
    "`print(x.data)`\n",
    "\n",
    "`print(x.dtype)`\n",
    "\n",
    "`print(x.shape)`\n",
    "\n",
    "`print(x.strides)`\n",
    "\n",
    "Now you can use the array, mostly by doing maths on them. Here are a few examples:\n",
    "\n",
    "Add, subtract, multiply and divide x and y:\n",
    "\n",
    "`np.add(x,y)`\n",
    "\n",
    "`np.subtract(x,y)`\n",
    "\n",
    "`np.multiply(x,y)`\n",
    "\n",
    "`np.divide(x,y)`\n",
    "\n",
    "You can experiment with a few more NumPy operations on your own in the Activities that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/aml-logo.png\"><b>Activity - Programming with NumPy</b></p>\n",
    "\n",
    "Open the **03_WorkingWithData.py** file and enter the code you find for section 3.1a. The exercises will be marked out using comments:\n",
    "\n",
    "`# <TODO> - 3.1a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> - 3.1a NumPy Exercises\n",
    "# Create a NumPy 1-dimensional array consisting of three numbers.\n",
    "# Sum those numbers. \n",
    "# Add three more numbers as an additional dimension to the array.\n",
    "# Sum the two dimensions over the rows.\n",
    "# Sum the two dimensions over the columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Pandas</b></p>\n",
    "\n",
    "The primary library you'll use in working with data in Python is *Pandas* (the *Python Analysis Data Library*). Pandas provides many methods and properties that you can work with for your data, and it also has other data structures that make it more efficient to work with data.\n",
    "\n",
    "Just as in NumPy, use the `import` statement to load the Pandas Library:\n",
    "\n",
    "`import pandas as pd`\n",
    "\n",
    "Once the library is in memory, you start using it by creating a *dataframe* - the primary object Pandas works with. A dataframe is a mixed-type structure that looks similar to a SQL Table, and is very efficient. You can assign almost any data to a dataframe - here's an example that creates a dataframe by reading a comma-separated file:\n",
    "\n",
    "`my_df = pd.read_csv('../data/data.csv')`\n",
    "\n",
    "This illustrates one way of ingesting data, and in a moment you'll see a few more. Pandas has a lot of data sources it can work with, from the Clipboard to various filetypes. Here's a short list:\n",
    "\n",
    "- Flat Files\n",
    "\n",
    "- Clipboard\n",
    "\n",
    "- Excel\n",
    "\n",
    "- JSON\n",
    "\n",
    "- HTML\n",
    "\n",
    "- HDFStore: PyTables (HDF5)\n",
    "\n",
    "- Feather\n",
    "\n",
    "- Parquet\n",
    "\n",
    "- SAS\n",
    "\n",
    "- SQL\n",
    "\n",
    "- Google BigQuery\n",
    "\n",
    "- STATA\n",
    "\n",
    "...among others.\n",
    "\n",
    "Now with the dataframe (`my_df`) loaded, it's an object you can work with. If you just type the name of the dataframe, you'll get back the data in the \"table\".\n",
    "\n",
    "Pandas has a lot of functions that allow you to work with data after you've inspected it. To work with datasets like you would in an RDBMS, here are a few examples.\n",
    "\n",
    "Starting with an equivalent (kind of) to the SELECT statement in SQL, you can project a column with the statement `my_df[col]`. Use a comma and include other columns to form *column, column*. These will come back as a new dataframe.\n",
    "\n",
    "If you want to use an ordinal position use `my_df.iloc[0]`. \n",
    "\n",
    "f you know the index you want, use `my_df.loc['index_one']`.\n",
    "\n",
    "If you want the whole row, use `my_df.iloc[0,:]` (for the first row).\n",
    "\n",
    "For a WHERE clause, use the comparison tokens you saw earlier. For instance, to get the months lower than October, use `my_df[my_df[month] > 9]`. \n",
    "\n",
    "For ORDER BY, use the sort_values function. This command sorts the first column of the dataframe in ascending order: `my_df.sort_values(col1,ascending=True)`.\n",
    "\n",
    "Moving on to JOIN operations, you have the ability to use multiple kinds of joins - for instance, the statement `my_df.join(my_df2,on=col1,how='inner')` joins the two dataframes `my_df` and `my_df2' on the column *col1* (which must exist in both dataframes).\n",
    "\n",
    "There's a lot more you can do with Pandas, including a lot of data cleaning operations that you'll use for Machine Learning and other Data Science tasks. You'll experiment with this in your Activities.\n",
    "\n",
    "Want to learn more? Check this reference: https://github.com/jvns/pandas-cookbook \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/aml-logo.png\"><b>Activity - Programming with Pandas</b></p>\n",
    "\n",
    "Open the **03_WorkingWithData.py** file and enter the code you find for section 3.1b. The exercises will be marked out using comments:\n",
    "\n",
    "`# <TODO> - 3.1b`\n",
    "\n",
    "(Note: Use the Cheat-sheets in the `./assets` directory in the exercise that follows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> - 3.1b Pandas Exercises\n",
    "# Use the Pandas library, and alias it as pd:\n",
    "\n",
    "# Show the first five values of long_series:\n",
    "long_series = pd.Series(np.random.randn(1000))\n",
    "\n",
    "# Read the file CATelcoCustomerChurnTrainingSample.csv from the ./data directory\n",
    "# into a Pandas Data Frame:\n",
    "\n",
    "# Explore the Data Frame you just created with Pandas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.3 Data Ingestion</b></p>\n",
    "\n",
    "Python has many ways to read data in (*sometimes into memory, sometimes streaming as it reads it*) built right in to the standard libraries. Other Libraries, such as Pandas and NumPy, have their own way of reading in data.\n",
    "\n",
    "In any case, the data is assigned to a data family or *structure*, which you learned about earlier. Depending on which Library you are using, you'll pick a data structure that makes the most sense for how you want to work with it. For instance, Pandas uses a dataframe as the primary data structure it works with. This is why it's important to know the data types, so that you understand what stucture you need to perform your desired operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Reading from Files</b></p>\n",
    "\n",
    "Many times the data you are looking for is in storage, either locally or remotely. *File-source* based data is loosely defined as whatever data the operating system can reach natively.\n",
    "\n",
    "*NOTE:* This means that when you write your code, it's important to know where it will run. Python is an *interpreted* language, which means that it will run on a given platform in a certain way. If you load data from a Windows file system, and it gets deployed to a Linux system, you need to make sure the file-paths check for validity.\n",
    "\n",
    "You've already seen how to read data with Pandas. For the built-in Python library, you most often use the csv reader on comma-separated value data. To use it, import the `csv` module. From there, you can use a \"with\" block to process the file. This example opens a file, uses an if statement to process each line, and if the line contains \"carrot\", prints the ingredient, the type of carrot (shredded, sliced, etc.), and the amount for the recipe:\n",
    "\n",
    "<pre>\n",
    "import csv\n",
    "with open('mydata.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if row['ingredient'] == 'carrot':\n",
    "            print(row['ingredient'] ,row ['type'],row ['amount'])\n",
    "</pre>\n",
    "\n",
    "(Note the indentation - very important!)\n",
    "\n",
    "The csv reader has a \"dialect\" modifier so that you can work with CSV files that are stored in a particular way - use the `help()` function to learn more.\n",
    "\n",
    "Reference: https://realpython.com/python-csv/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Working with Data in Databases</b></p>\n",
    "\n",
    "Python has Libraries available that allow you to connect to a Relational Database Management System (RDBMS). the `pydobc` Library is one of the most widely used, and works well with Microsoft's SQL Server. You can read more about pyodbc and download it here: https://docs.microsoft.com/en-us/sql/connect/python/pyodbc/python-sql-driver-pyodbc?view=sql-server-2017 \n",
    "\n",
    "Once you install it (more on installing Libraries later), you once again import it, and then set up your connection. You then use the connection to send a query, returning a dataset, or updating data if that's what you're going for. Here's an example:\n",
    "\n",
    "<pre>\n",
    "    import pyodbc\n",
    "\n",
    "    server = 'tcp:myserver.database.windows.net'\n",
    "    # Some other example server values are\n",
    "    # server = 'localhost\\sqlexpress' for a named instance\n",
    "    # server = 'myserver,port' to specify an alternate port\n",
    "\n",
    "    database = 'mydb'\n",
    "    username = 'myusername'\n",
    "    password = 'mypassword'\n",
    "\n",
    "    cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ \n",
    "    password)\n",
    "\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    # Sample select query\n",
    "    cursor.execute(\"SELECT @@version;\")\n",
    "    row = cursor.fetchone()\n",
    "\n",
    "    while row: \n",
    "\n",
    "        print row[0]\n",
    "        row = cursor.fetchone()\n",
    "\n",
    "    # Sample insert query\n",
    "\n",
    "    cursor.execute(\"INSERT SalesLT.Product (Name, ProductNumber, StandardCost, ListPrice, SellStartDate) OUTPUT INSERTED.ProductID \n",
    "    VALUES ('SQL Server Express New 20', 'SQLEXPRESS New 20', 0, 0, CURRENT_TIMESTAMP )\")\n",
    "\n",
    "    row = cursor.fetchone()\n",
    "    while row:\n",
    "        print 'Inserted Product key is ' + str(row[0]) \n",
    "        row = cursor.fetchone()\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Data in Other Sources</b></p>\n",
    "\n",
    "Many other data sources, such as cloud databases and network streams, also have ways of connecting from Python. Even web pages can be used as data sources. One of the primary Libraries for working with web data is *Beautiful Soup*, [which you can find here](https://www.crummy.com/software/BeautifulSoup/). You normally need to connect to the web page first, so for that you use another import, using `requests`, or perhaps `urllib` or `urllib2`. \n",
    "\n",
    "Here's an example of reading a web page and printing all the links it has:\n",
    "\n",
    "<pre>\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    html_doc  = requests.get(\"http://coolwebpage.com\")\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    print(soup.get_text())\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/aml-logo.png\"><b>Activity - Data Ingestion</b></p>\n",
    "\n",
    "Open the **03_WorkingWithData.py** file and enter the code you find for section 3.2. The exercises will be marked out using comments:\n",
    "\n",
    "`# <TODO> - 3.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> - 3.2 Data Ingestion\n",
    "# Read customer data from the ./data/CATelcoCustomerChurnTrainingSample.csv\n",
    "# into a data frame called df using pandas:\n",
    "\n",
    "# Show the data in the Data Frame:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.4 Data Inspection</b></p>\n",
    "\n",
    "After the data is loaded into a structure, the first step in analytics is to examine the data. You've already seen how to display the data using Pandas, and it's one of the best libraries for data exploration as well.\n",
    "\n",
    "Analytics professionals often start with the basics of the statistical layout of the numeric data in a dataset. If you want to see the basic statistics of your data stored in a dataframe called *my_df*, type `my_df.describe()`.  \n",
    "\n",
    "You'll also want to determine the amount of data you're working with. To do that, type `my_df.shape` to get the number of rows and columns in a dataframe. \n",
    "\n",
    "Typing `my_df.head(n)` gives you `n` first rows of the data, or use `my_df.tail(n)` to get the end number of rows returned. \n",
    "\n",
    "Another way to see the \"shape\" of the data is to use `my_df.info()` to see the index, datatypes and memory information for the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/aml-logo.png\"><b>Activity - Data Inspection</b></p>\n",
    "\n",
    "Open the **03_WorkingWithData.py** file and enter the code you find for section 3.3. The exercises will be marked out using comments:\n",
    "\n",
    "`# <TODO> - 3.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> - 3.2 Data Ingestion\n",
    "# Read customer data from the ./data/CATelcoCustomerChurnTrainingSample.csv\n",
    "# into a data frame called df using pandas:\n",
    "\n",
    "# <TODO> - 3.3 Data Inspection\n",
    "# Ensure that you have 29 columns and 20,468 rows loaded\n",
    "print('There should be 20468 observations of 29 variables:')\n",
    "\n",
    "# Explore the df Dataframe, using at least a five-number statistical summary.\n",
    "# NOTE: Your exploration may be much different - you will show this data\n",
    "# using graphs in the next exercise. \n",
    "\n",
    "# Show the size and shape of the data:\n",
    "\n",
    "# Show the first and last 10 rows:\n",
    "\n",
    "# Show the dataframe structure:\n",
    "\n",
    "# Check for missing values:\n",
    "print('Missing values: ', '\\n')\n",
    "\n",
    "# perform a simple statistical display:    \n",
    "print('Dataframe Statistics: ', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.4 Graphing</b></p>\n",
    "\n",
    "Examining the data in tabular format won't give you all you need to evaluate and interpret it. It is very useful to display the data in a graphical format, and once again you'll turn to Libraries to do that. There are many Libraries for graphing data in Python, and more are written constantly. The  primary Libraries you should be familiar with are MatPlotLib and ggplot.\n",
    "\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Graphing Data with MatPlotLib</b></p>\n",
    "\n",
    "MatPlotLib is quite old, bu it’s the most widely used graphical library for plotting in Python. It borrowed much of it's design  from an industry commercial standard called MATLAB. Many other Libraries are built on top of MatPlotLib or simply work along side it.\n",
    "\n",
    "Take a look at an example of a histogram plot with MatPlotLib:\n",
    "\n",
    "<pre>\n",
    "    import matplotlib\n",
    "    from numpy.random import randn\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "    def to_percent(y, position):\n",
    "        # Ignore the passed in position. This has the effect of scaling the default\n",
    "        # tick locations.\n",
    "        s = str(100 * y)\n",
    "\n",
    "        # The percent symbol needs escaping in latex\n",
    "        if matplotlib.rcParams['text.usetex'] is True:\n",
    "            return s + r'$\\%$'\n",
    "        else:\n",
    "            return s + '%'\n",
    "\n",
    "    x = randn(5000)\n",
    "\n",
    "    # Make a normed histogram. It'll be multiplied by 100 later.\n",
    "    plt.hist(x, bins=50, normed=True)\n",
    "\n",
    "    # Create the formatter using the function to_percent. This multiplies all the\n",
    "    # default labels by 100, making them all percentages\n",
    "    formatter = FuncFormatter(to_percent)\n",
    "\n",
    "    # Set the formatter\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    plt.show()\n",
    "</pre>\n",
    "\n",
    "![](../graphics/MatPlotLib.png)\n",
    "\n",
    "Of course, MatPlotLib can do so much more. [Take a look at this reference from the documentation which goes deeper.](https://matplotlib.org/examples/index.html)\n",
    "\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/checkbox.png\"><b>Graphing with ggplot</b></p>\n",
    "\n",
    "The ggplot library is also used in the R language (in a newer version called *ggplot2*). It follows the guidelines from the *Grammar of Graphics* reference work. The commands in ggplot layer the graphical components. You'll make a base graphic, and even after you create the chart you add axes, a line, add a trendline, coloring and more.\n",
    "\n",
    "Here's an example of a plot using the ggplot Library, with the mtcars sample dataset. Notice how it \"builds\" on the plot so that it's fairly easy to see how it represents each part:\n",
    "<pre>\n",
    "    from ggplot import *\n",
    "\n",
    "    p = ggplot(aes(x='mpg'), data=mtcars)\n",
    "    p += geom_histogram()\n",
    "    p += xlab(\"Miles per Gallon\")\n",
    "    p += ylab(\"# of Cars\")\n",
    "    p\n",
    "</pre>\n",
    "\n",
    "![](../graphics/ggplot.png)\n",
    "\n",
    "[Check out the official documentation for many more examples.](https://github.com/yhat/ggpy/tree/master/docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/aml-logo.png\"><b>Activity - Graphing</b></p>\n",
    "\n",
    "Open the **03_WorkingWithData.py** file and enter the code you find for section 3.4. The exercises will be marked out using comments:\n",
    "\n",
    "`# <TODO> - 3.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> - 3.4 Graphing\n",
    "# Using any graphical library or representation you like, create three separate graphs\n",
    "# that best illustrate the data layout of the dataframe you just created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.6 Altering Data</b></p>\n",
    "\n",
    "Most data isn't \"clean\" by default. It's either in the wrong format, missing values, or isn't all structured the way you need it. For this type of work, there are two basic tasks you should learn: Regular Expressions and once again, Pandas. You won't cover an exercise on data editing in this section; instead you'll see an example of that as part of a Machine Learning exercise.\n",
    "\n",
    "You can use Regular Expressions in Python to make a lot of your changes - you can read more about that here: https://docs.python.org/3/library/re.html\n",
    "\n",
    "But most of the time you'll be using Pandas to make those changes. You can read more about that here: https://tomaugspurger.github.io/modern-5-tidy.html \n",
    "\n",
    "And of course there are lots of other things to know about altering data. Read this resource for more: https://www.springboard.com/blog/data-wrangling/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/cortanalogo.png\"><b>3.7 Machine Learning and AI</b></p>\n",
    "\n",
    "A full course on Machine Learning (and one of its applications, Artificial Intelligence), is long and involved. Machine Learning involves evaluating data for *features* (columns) that can create *labels* (predictions or classifications). You do this by using a collection of historical data, and selecting the most predictive features and applying one or more algorithms to that data. You get back a *model* (which is kind of like a function) that you can send new data to for a prediction. This is a bit of an oversimplification of course, but it will serve you well as you work through this course. For a more comprehensive discussion on Data Science and Machine Learning with Python, check out this reference: https://notebooks.azure.com/jakevdp/libraries/PythonDataScienceHandbook\n",
    "\n",
    "There are a few \"families\" of problems you can solve with a Machine Learning Solution:\n",
    "\n",
    "<p>\n",
    "<img src=\"../graphics/MLCapabilities.png\" width=\"500\">\n",
    "<p>\n",
    "\n",
    "While it's tempting to start with the algorithms and the outputs, it's actually more important to understand the general process of a Data Science project. To do that, you can use the Team Data Science Process - in fact, you have been studying many of these steps already:\n",
    "\n",
    "<p>\n",
    "<img src=\"../graphics/tdsp.png\" width=\"500\">\n",
    "<p>\n",
    "\n",
    "Each of these phases has a specific set of steps you follow to complete them:\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p>\n",
    " \n",
    "<h4>Phase One - Business Understanding</h4>\n",
    "\n",
    "In the Business Understanding Phase the team determines the prediction or categorical work your organization wants to create. You'll also set up your project planning documents, locate your initial data source locations, and set up the environment you will use to create and operationalize your models. This phase involves a great deal of coordination among the team and the broader organization.\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-business-understanding)\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "<h4>Phase Two - Data Acquisition and Understanding</h4>\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-data)\n",
    "\n",
    "The Data Aquisition and Understanding phase of the TDSP you ingest or access data from various locations to answer the questions the organization has asked. In most cases, this data will be in multiple locations. Once the data is ingested into the system, you’ll need to examine it to see what it holds. All data needs cleaning, so after the inspection phase, you’ll replace missing values, add and change columns. You’ve already seen the Libraries you'll need to work with for Data Wrangling - Pandas being the most common in use.\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "<h4>Phase Three - Modeling</h4>\n",
    "\n",
    "In this phase, you will create the experiment runs, perform feature engineering, and run experiments with various settings and parameters. After selecting the best performing run, you will create a trained model and save it for operationalization in the next phase. This modeling is done with yet another set of Python Libraries - the most common being SciKit Learn and TensorFlow <TODO>: References, among others. You'll see this in action in just a bit.\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-modeling)\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "<h4>Phase Four - Deployment</h4>\n",
    "\n",
    "In this phase you will take the trained model and any other necessary assets and deploy them to a system that will respond to API requests.\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-deployment)\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "<h4>Phase Five - Customer Acceptance</h4>\n",
    "\n",
    "The final phase involves testing the model predictions on real-world queries to ensure that it meets all requirements. In this phase you also document the project so that all parameters are well-known. Finally, a mechanism is created to re-train the model.\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-acceptance)\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "As you can see, there are quite a few things to do to work with Python in a Data Science Machine Learning project. Rather than have you create an entire solution, there is one you can examine to see each phase. You'll do that next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO> - 3.5 Machine Learning and AI\n",
    "# Review the following code, observing what it does. \n",
    "\n",
    "# 1 - Setup - Get everything up to date, and add any pips you want here\n",
    "# Import Libraries for the Customer Churn Prediction Labs - Change for other uses\n",
    "\n",
    "# Serializing output/input\n",
    "import pickle\n",
    "\n",
    "# Libraries for training and scoring\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Data and Numeric Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Working with files\n",
    "import csv\n",
    "\n",
    "#/ 1 - Setup \n",
    "\n",
    "#2 - Read data and verify\n",
    "# Read customer data from a single file\n",
    "df = pd.read_csv('./data/CATelcoCustomerChurnTrainingSample.csv') \n",
    "\n",
    "# Ensure that you have 29 columns and 20,468 rows loaded\n",
    "print('There should be 20468 obervations of 29 variables:')\n",
    "print(df.shape, '\\n')\n",
    "\n",
    "# Optional - Instead, read the data from source:\n",
    "# https://github.com/Azure/MachineLearningSamples-ChurnPrediction/blob/master/data/CATelcoCustomerChurnTrainingSample.csv \n",
    "#/ 2 - Read Data\n",
    "\n",
    "# 2.1 - Explore Data\n",
    "# Explore the df Dataframe, using at least a five-number statistical summary.\n",
    "# NOTE: Your exploration may be much different - experiment with graphics as well.\n",
    "\n",
    "# Show the size and shape of data:\n",
    "print('The size of the data is: %d rows and  %d columns' % df.shape, '\\n')\n",
    "\n",
    "# Show the first and last 10 rows\n",
    "print('First ten rows of the data: ')\n",
    "print(df.head(10), '\\n')\n",
    "print('Last ten rows of the data: ')\n",
    "print(df.tail(10), '\\n')\n",
    "\n",
    "# Show the dataframe structure:\n",
    "print('Dataframe Structure: ', '\\n')\n",
    "print(df.info(), '\\n')\n",
    "\n",
    "# Check for missing values:\n",
    "print('Missing values: ', '\\n')\n",
    "print(df.apply(lambda x: sum(x.isnull()),axis=0), '\\n') \n",
    "\n",
    "# perform a simple statistical display:    \n",
    "print('Dataframe Statistics: ', '\\n')\n",
    "print(df.describe(), '\\n')\n",
    "\n",
    "#/ 2.1\n",
    "\n",
    "# 3.0 - Customer Churn Prediction Experiment\n",
    "# For completeness of this example, let's re-import our libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# We'll re-load the data as \"CustomerDataFrame\"\n",
    "CustomerDataFrame = pd.read_csv('data/CATelcoCustomerChurnTrainingSample.csv')\n",
    "\n",
    "# Fill all NA values with 0:\n",
    "CustomerDataFrame = CustomerDataFrame.fillna(0)\n",
    "\n",
    "# Drop all duplicate observations:\n",
    "CustomerDataFrame = CustomerDataFrame.drop_duplicates()\n",
    "\n",
    "# We don't need the 'year\" or 'month' variables\n",
    "CustomerDataFrame = CustomerDataFrame.drop('year', 1)\n",
    "CustomerDataFrame = CustomerDataFrame.drop('month', 1)\n",
    "\n",
    "# Implement One-Hot Encoding for this model (https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) \n",
    "columns_to_encode = list(CustomerDataFrame.select_dtypes(include=['category','object']))\n",
    "dummies = pd.get_dummies(CustomerDataFrame[columns_to_encode]) #\n",
    "\n",
    "# Drop the original categorical columns:\n",
    "CustomerDataFrame = CustomerDataFrame.drop(columns_to_encode, axis=1) # \n",
    "\n",
    "# Re-join the dummies frame to the original data:\n",
    "CustomerDataFrame = CustomerDataFrame.join(dummies)\n",
    "\n",
    "# Show the new columns in the joined dataframe:\n",
    "print(CustomerDataFrame.columns, '\\n')\n",
    "\n",
    "# Experiment using Naive Bayes:\n",
    "nb_model = GaussianNB()\n",
    "random_seed = 42\n",
    "split_ratio = .3\n",
    "train, test = train_test_split(CustomerDataFrame, random_state = random_seed, test_size = split_ratio)\n",
    "\n",
    "target = train['churn'].values\n",
    "train = train.drop('churn', 1)\n",
    "train = train.values\n",
    "nb_model.fit(train, target)\n",
    "\n",
    "expected = test['churn'].values\n",
    "test = test.drop('churn', 1)\n",
    "predicted = nb_model.predict(test)\n",
    "\n",
    "# Print out the Naive Bayes Classification Accuracy:\n",
    "print(\"Naive Bayes Classification Accuracy\", accuracy_score(expected, predicted))\n",
    "\n",
    "# Experiment using Decision Trees:\n",
    "dt_model = DecisionTreeClassifier(min_samples_split=20, random_state=99)\n",
    "dt_model.fit(train, target)\n",
    "predicted = dt_model.predict(test)\n",
    "\n",
    "# Print out the Decision Tree Accuracy:\n",
    "print(\"Decision Tree Classification Accuracy\", accuracy_score(expected, predicted))\n",
    "\n",
    "#/ 3.0\n",
    "\n",
    "# 4.0a - Create the Model File\n",
    "# serialize the best performing model on disk\n",
    "print (\"Serialize the model to a model.pkl file in the root\")\n",
    "ModelFile = open('./model.pkl', 'wb')\n",
    "pickle.dump(dt_model, ModelFile)\n",
    "ModelFile.close()\n",
    "#/ 4.0a\n",
    "\n",
    "# 4.0b - Operationalization: Scoring the calls to the model\n",
    "# Prepare the web service definition before deploying\n",
    "# Import for the pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# load the model file\n",
    "global model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Import for handling the JSON file\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Set up a sample \"call\" from a client:\n",
    "input_df = \"{\\\"callfailurerate\\\": 0, \\\"education\\\": \\\"Bachelor or equivalent\\\", \\\"usesinternetservice\\\": \\\"No\\\", \\\"gender\\\": \\\"Male\\\", \\\"unpaidbalance\\\": 19, \\\"occupation\\\": \\\"Technology Related Job\\\", \\\"year\\\": 2015, \\\"numberofcomplaints\\\": 0, \\\"avgcallduration\\\": 663, \\\"usesvoiceservice\\\": \\\"No\\\", \\\"annualincome\\\": 168147, \\\"totalminsusedinlastmonth\\\": 15, \\\"homeowner\\\": \\\"Yes\\\", \\\"age\\\": 12, \\\"maritalstatus\\\": \\\"Single\\\", \\\"month\\\": 1, \\\"calldroprate\\\": 0.06, \\\"percentagecalloutsidenetwork\\\": 0.82, \\\"penaltytoswitch\\\": 371, \\\"monthlybilledamount\\\": 71, \\\"churn\\\": 0, \\\"numdayscontractequipmentplanexpiring\\\": 96, \\\"totalcallduration\\\": 5971, \\\"callingnum\\\": 4251078442, \\\"state\\\": \\\"WA\\\", \\\"customerid\\\": 1, \\\"customersuspended\\\": \\\"Yes\\\", \\\"numberofmonthunpaid\\\": 7, \\\"noadditionallines\\\": \\\"\\\\\\\\N\\\"}\"\n",
    "\n",
    "# Cleanup \n",
    "input_df_encoded = json.loads(input_df)\n",
    "input_df_encoded = pd.DataFrame([input_df_encoded], columns=input_df_encoded.keys())\n",
    "input_df_encoded = input_df_encoded.drop('year', 1)\n",
    "input_df_encoded = input_df_encoded.drop('month', 1)\n",
    "input_df_encoded = input_df_encoded.drop('churn', 1)\n",
    "\n",
    "# Pre-process scoring data consistent with training data\n",
    "columns_to_encode = ['customersuspended', 'education', 'gender', 'homeowner', 'maritalstatus', 'noadditionallines', 'occupation', 'state', 'usesinternetservice', 'usesvoiceservice']\n",
    "dummies = pd.get_dummies(input_df_encoded[columns_to_encode])\n",
    "input_df_encoded = input_df_encoded.join(dummies)\n",
    "input_df_encoded = input_df_encoded.drop(columns_to_encode, axis=1)\n",
    "\n",
    "columns_encoded = ['age', 'annualincome', 'calldroprate', 'callfailurerate', 'callingnum',\n",
    "       'customerid', 'monthlybilledamount', 'numberofcomplaints',\n",
    "       'numberofmonthunpaid', 'numdayscontractequipmentplanexpiring',\n",
    "       'penaltytoswitch', 'totalminsusedinlastmonth', 'unpaidbalance',\n",
    "       'percentagecalloutsidenetwork', 'totalcallduration', 'avgcallduration',\n",
    "       'customersuspended_No', 'customersuspended_Yes',\n",
    "       'education_Bachelor or equivalent', 'education_High School or below',\n",
    "       'education_Master or equivalent', 'education_PhD or equivalent',\n",
    "       'gender_Female', 'gender_Male', 'homeowner_No', 'homeowner_Yes',\n",
    "       'maritalstatus_Married', 'maritalstatus_Single', 'noadditionallines_\\\\N',\n",
    "       'occupation_Non-technology Related Job', 'occupation_Others',\n",
    "       'occupation_Technology Related Job', 'state_AK', 'state_AL', 'state_AR',\n",
    "       'state_AZ', 'state_CA', 'state_CO', 'state_CT', 'state_DE', 'state_FL',\n",
    "       'state_GA', 'state_HI', 'state_IA', 'state_ID', 'state_IL', 'state_IN',\n",
    "       'state_KS', 'state_KY', 'state_LA', 'state_MA', 'state_MD', 'state_ME',\n",
    "       'state_MI', 'state_MN', 'state_MO', 'state_MS', 'state_MT', 'state_NC',\n",
    "       'state_ND', 'state_NE', 'state_NH', 'state_NJ', 'state_NM', 'state_NV',\n",
    "       'state_NY', 'state_OH', 'state_OK', 'state_OR', 'state_PA', 'state_RI',\n",
    "       'state_SC', 'state_SD', 'state_TN', 'state_TX', 'state_UT', 'state_VA',\n",
    "       'state_VT', 'state_WA', 'state_WI', 'state_WV', 'state_WY',\n",
    "       'usesinternetservice_No', 'usesinternetservice_Yes',\n",
    "       'usesvoiceservice_No', 'usesvoiceservice_Yes']\n",
    "\n",
    "# Now that they are encoded, some values will be \"empty\". Fill those with 0's:\n",
    "for column_encoded in columns_encoded:\n",
    "    if not column_encoded in input_df_encoded.columns:\n",
    "        input_df_encoded[column_encoded] = 0\n",
    "\n",
    "# Return final prediction\n",
    "pred = model.predict(input_df_encoded)\n",
    "\n",
    "# (In production you would replace Print() statement here with some sort of return to JSON)\n",
    "print('JSON sent to the prediction Model:', '\\n')\n",
    "print(input_df, '\\n')\n",
    "print('For the JSON string sent from the client, The prediction is returned as more JSON (0 = No churn, 1 = Churn):', '\\n')\n",
    "print(json.dumps(str(pred[0])))\n",
    "\n",
    "#/ 4.0b\n",
    "\n",
    "# EOF: 03_WorkingWithData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"../graphics/thinking.jpg\"><b>For Further Study</b></p>\n",
    "\n",
    "- [Python Docs for Data Types](https://docs.python.org/2/tutorial/datastructures.html#)\n",
    "\n",
    "Next, Continue to *04 Environments and Deployment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
